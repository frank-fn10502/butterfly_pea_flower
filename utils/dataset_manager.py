import os
import re
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from config import *

class DatasetManager:
    def __init__(self):
        """ åˆå§‹åŒ– DatasetManagerï¼Œç¢ºä¿æ•¸æ“šé›†å¯ç”¨ """
        self.dataset_csv = CSV_PATH
        self.dataset_dir = DATASET_DIR
        self.train_df = None
        self.val_df = None
        self.num_train_samples = 0
        self.num_valid_samples = 0

        # **ç¢ºä¿æ•¸æ“šé›† CSV å­˜åœ¨ï¼Œå¦å‰‡ç”Ÿæˆ**
        if not os.path.exists(self.dataset_csv):
            print("âš ï¸ æ‰¾ä¸åˆ° `ph_dataset.csv`ï¼Œæ­£åœ¨é‡æ–°ç”Ÿæˆ...")
            self.generate_csv()
        else:
            print(f"âœ… ç™¼ç¾ç¾æœ‰çš„ `{self.dataset_csv}`ï¼Œç›´æ¥ä½¿ç”¨ï¼")

        self.load_and_split_data()

    def generate_csv(self):
        """ å¾è³‡æ–™å¤¾è®€å–åœ–ç‰‡æª”æ¡ˆï¼Œæå– PH å€¼ï¼Œä¸¦å­˜ç‚º `ph_dataset.csv` """
        ph_pattern = re.compile(r'PH(\d+\.?\d*)')
        file_list = []
        ph_values = []

        for root, _, files in os.walk(self.dataset_dir):
            for filename in files:
                if filename.lower().endswith((".jpg", ".png", ".jpeg")):
                    match = ph_pattern.search(filename)
                    if match:
                        ph_value = float(match.group(1))
                        file_path = os.path.join(root, filename)
                        file_list.append(file_path)
                        ph_values.append(ph_value)

        df = pd.DataFrame({"filename": file_list, "ph_value": ph_values})
        df.to_csv(self.dataset_csv, index=False)
        print(f"âœ… `ph_dataset.csv` ç”Ÿæˆå®Œæˆï¼Œç¸½å…± {len(df)} ç­†è³‡æ–™ï¼")

    def load_and_split_data(self):
        """ è®€å– `ph_dataset.csv` ä¸¦åˆ‡åˆ†è¨“ç·´/é©—è­‰é›† """
        df = pd.read_csv(self.dataset_csv)
        self.train_df = df.sample(frac=TRAIN_SPLIT, random_state=42)
        self.val_df = df.drop(self.train_df.index)

        self.num_train_samples = len(self.train_df)
        self.num_valid_samples = len(self.val_df)

        print(f"ğŸ“Š è¨“ç·´æ¨£æœ¬: {self.num_train_samples}ï¼Œé©—è­‰æ¨£æœ¬: {self.num_valid_samples}")

        # **å»ºç«‹è¨“ç·´èˆ‡é©—è­‰æ•¸æ“šç”Ÿæˆå™¨**
        self.train_generator = self.create_train_generator(self.train_df, shuffle=True)
        self.val_generator = self.create_valid_generator(self.val_df, shuffle=False)

    def create_train_generator(self, dataframe, shuffle):
        """ å»ºç«‹è¨“ç·´è³‡æ–™çš„ Data Augmentation """
        train_datagen = ImageDataGenerator(
            rescale=1.0 / 255,
            rotation_range=15,        # âœ… éš¨æ©Ÿæ—‹è½‰ 15 åº¦
            width_shift_range=0.1,    # âœ… éš¨æ©Ÿæ°´å¹³å¹³ç§» 10%
            height_shift_range=0.1,   # âœ… éš¨æ©Ÿå‚ç›´å¹³ç§» 10%
            zoom_range=0.2,           # âœ… éš¨æ©Ÿç¸®æ”¾ 20%
            horizontal_flip=True      # âœ… éš¨æ©Ÿæ°´å¹³ç¿»è½‰
        )
        return train_datagen.flow_from_dataframe(
            dataframe,
            x_col="filename",
            y_col="ph_value",
            target_size=IMG_SIZE,
            batch_size=BATCH_SIZE,
            class_mode='raw',
            shuffle=shuffle
        )

    def create_valid_generator(self, dataframe, shuffle):
        """ å»ºç«‹é©—è­‰è³‡æ–™çš„ç”Ÿæˆå™¨ï¼ˆä¸åš Data Augmentationï¼‰ """
        valid_datagen = ImageDataGenerator(rescale=1.0 / 255)
        return valid_datagen.flow_from_dataframe(
            dataframe,
            x_col="filename",
            y_col="ph_value",
            target_size=IMG_SIZE,
            batch_size=BATCH_SIZE,
            class_mode='raw',
            shuffle=shuffle
        )
